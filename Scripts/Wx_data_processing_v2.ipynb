{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "   <style>     div#notebook-container    { width: 100%; }     div#menubar-container     { width: 85%; }     div#maintoolbar-container { width: 99%; } </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make the screen bigger!\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"   <style>     div#notebook-container    { width: 100%; }     div#menubar-container     { width: 85%; }     div#maintoolbar-container { width: 99%; } </style>\n",
    "\"\"\"))\n",
    "\n",
    "# import all our libraries (if this cell throws an error you may need to reinstall a library)\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil   \n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "%run Functions/Funcs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Aasu-CR300wx.dat to Aasu-CR300wx.csv\n",
      "Converting Afono-wx-060519.dat to Afono-wx-060519.csv\n",
      "Converting Poloa-CR300wx.dat to Poloa-CR300wx.csv\n",
      "Converting Vaipito-CR300wx.dat to Vaipito-CR300wx.csv\n",
      "Converting Aasu -Weather-0616161.xlsx to Aasu -Weather-0616161.csv\n",
      "Converting Aasu -Weather-0616162.xlsx to Aasu -Weather-0616162.csv\n",
      "Converting Aasu Downloaded01182016.xlsx to Aasu Downloaded01182016.csv\n",
      "Converting Aasu Wx 020917.xlsx to Aasu Wx 020917.csv\n",
      "Converting Aasu Wx 092816.xlsx to Aasu Wx 092816.csv\n",
      "Converting Aasu Wx 2016 & Notes (2).xlsx to Aasu Wx 2016 & Notes (2).csv\n",
      "Converting Aasu Wx 2016 & Notes (3).xlsx to Aasu Wx 2016 & Notes (3).csv\n",
      "Converting Aasu Wx 2016 & Notes.xlsx to Aasu Wx 2016 & Notes.csv\n",
      "Converting Aasu _Downloaded10072015.xlsx to Aasu _Downloaded10072015.csv\n",
      "Converting Aasu _Wx_071916.xlsx to Aasu _Wx_071916.csv\n",
      "Converting Aasu-Wx-013018.xlsx to Aasu-Wx-013018.csv\n",
      "Converting Afono-Wx-013018.xlsx to Afono-Wx-013018.csv\n",
      "Converting Afono-Wx-102418.xlsx to Afono-Wx-102418.csv\n",
      "Converting Alava Wx 020917.xlsx to Alava Wx 020917.csv\n",
      "Converting Alava Wx 091317.xlsx to Alava Wx 091317.csv\n",
      "Converting Alava Wx 092016.xlsx to Alava Wx 092016.csv\n",
      "Converting Alava Wx 2016 & Notes (2).xlsx to Alava Wx 2016 & Notes (2).csv\n",
      "Converting Alava Wx 2016 & Notes.xlsx to Alava Wx 2016 & Notes.csv\n",
      "Converting Alava Wx Raw1-2017 .xlsx to Alava Wx Raw1-2017 .csv\n",
      "Converting Alava Wx-110217.xlsx to Alava Wx-110217.csv\n",
      "Converting Alava _01042016 (2).xlsx to Alava _01042016 (2).csv\n",
      "Converting Alava _100815.xlsx to Alava _100815.csv\n",
      "Converting Alava-Wx-102418.xlsx to Alava-Wx-102418.csv\n",
      "Converting ASPA_solar  SR Wx 100616.xlsx to ASPA_solar  SR Wx 100616.csv\n",
      "Converting ASPA_solar Solar Rad-0616162.xlsx to ASPA_solar Solar Rad-0616162.csv\n",
      "Converting ASPA_solar Solar Wx 2016 (2).xlsx to ASPA_solar Solar Wx 2016 (2).csv\n",
      "Converting ASPA_solar Solar Wx 2016 (3).xlsx to ASPA_solar Solar Wx 2016 (3).csv\n",
      "Converting ASPA_solar Solar Wx 2016.xlsx to ASPA_solar Solar Wx 2016.csv\n",
      "Converting ASPA_solar SolarRad_071916.xlsx to ASPA_solar SolarRad_071916.csv\n",
      "Converting ASPA_solar Solar_Jan2016.xlsx to ASPA_solar Solar_Jan2016.csv\n",
      "Converting ASPA_solar SR 032817.xlsx to ASPA_solar SR 032817.csv\n",
      "Converting ASPA_solar SR 053117.xlsx to ASPA_solar SR 053117.csv\n",
      "Converting ASPA_solar SR 070917.xlsx to ASPA_solar SR 070917.csv\n",
      "Converting ASPA_solar Wx 091317.xlsx to ASPA_solar Wx 091317.csv\n",
      "Converting ASPA_solar-Wx-110217.xlsx to ASPA_solar-Wx-110217.csv\n",
      "Converting Fagaitua no rain wx 091317.xlsx to Fagaitua no rain wx 091317.csv\n",
      "Converting Fagaitua no rain-Wx-110217.xlsx to Fagaitua no rain-Wx-110217.csv\n",
      "Converting Fagaitua noRain 070917.xlsx to Fagaitua noRain 070917.csv\n",
      "Converting Fagaitua-Wx-102418.xlsx to Fagaitua-Wx-102418.csv\n",
      "Converting Fangaitua Wx station (2).xlsx to Fangaitua Wx station (2).csv\n",
      "Converting Poloa -Weather-0616162.xlsx to Poloa -Weather-0616162.csv\n",
      "Converting Poloa Wx 020917.xlsx to Poloa Wx 020917.csv\n",
      "Converting Poloa Wx 051917.xlsx to Poloa Wx 051917.csv\n",
      "Converting Poloa Wx 070917.xlsx to Poloa Wx 070917.csv\n",
      "Converting Poloa Wx 091317.xlsx to Poloa Wx 091317.csv\n",
      "Converting Poloa Wx 100316.xlsx to Poloa Wx 100316.csv\n",
      "Converting Poloa Wx 2016 & Notes (2).xlsx to Poloa Wx 2016 & Notes (2).csv\n",
      "Converting Poloa Wx 2016 & Notes (3).xlsx to Poloa Wx 2016 & Notes (3).csv\n",
      "Converting Poloa Wx 2016 & Notes.xlsx to Poloa Wx 2016 & Notes.csv\n",
      "Converting Poloa Wx-110217.xlsx to Poloa Wx-110217.csv\n",
      "Converting Poloa _Downloaded01192016.xlsx to Poloa _Downloaded01192016.csv\n",
      "Converting Poloa _Downloaded10072015.xlsx to Poloa _Downloaded10072015.csv\n",
      "Converting Poloa _Downloaded10072015_jul_oct.xlsx to Poloa _Downloaded10072015_jul_oct.csv\n",
      "Converting Poloa _Wx_071916.xlsx to Poloa _Wx_071916.csv\n",
      "Converting Poloa-Wx-102418.xlsx to Poloa-Wx-102418.csv\n",
      "Converting Vaipito -Weather-June061616.xlsx to Vaipito -Weather-June061616.csv\n",
      "Converting Vaipito -Weather-May.xlsx to Vaipito -Weather-May.csv\n",
      "Converting Vaipito Wx_071916.xlsx to Vaipito Wx_071916.csv\n",
      "Converting Vaipito _Wx09292015.xlsx to Vaipito _Wx09292015.csv\n",
      "Converting Vaipito _Wx201507.xlsx to Vaipito _Wx201507.csv\n",
      "Converting Vaipito _Wx20150722.xlsx to Vaipito _Wx20150722.csv\n",
      "Converting Vaipito _Wx20160119.xlsx to Vaipito _Wx20160119.csv\n",
      "Converting Vaipito _WxSD201508.xlsx to Vaipito _WxSD201508.csv\n",
      "Converting Vaipito _WxSD201509.xlsx to Vaipito _WxSD201509.csv\n",
      "Converting Vaipito-Weather-May2.xlsx to Vaipito-Weather-May2.csv\n",
      "Converting Vipito_Wx_071916.xlsx to Vipito_Wx_071916.csv\n"
     ]
    }
   ],
   "source": [
    "pristine_raw_data       = os.path.join('..', 'Raw_data/Weather_stations/Raw_data')\n",
    "Raw_data_copy  = os.path.join('.', 'Workspace/Raw_data_copy')\n",
    "Formatted_data = os.path.join('.', 'Workspace/Formatted_raw_data')\n",
    "\n",
    "# string to look for is anything with \"time\" in it for taking out trash headings\n",
    "p = re.compile('time', re.IGNORECASE) \n",
    "# Final data headings\n",
    "headings = ['DateTime', 'SRD_wpm2', 'HMD_pct', 'TMP_F', 'RNF_in', 'WND_deg', 'WNS_mph']\n",
    "\n",
    "\n",
    "# make a copy of the raw data\n",
    "files1 = os.listdir(pristine_raw_data) # make a list of all the files in the directory\n",
    "for f in files1:\n",
    "    shutil.copy(os.path.join(pristine_raw_data, f), Raw_data_copy)\n",
    "    \n",
    "# Change any dat files to csv     (they are just csvs it looks like so rename them locally)\n",
    "for i in os.listdir(Raw_data_copy):\n",
    "    if i.endswith('.dat'):\n",
    "        nam = os.path.splitext(i)[0]\n",
    "        os.rename(os.path.join(Raw_data_copy, i), os.path.join(Raw_data_copy, nam+\".csv\"))\n",
    "        print(\"Converting {} to {}.csv\".format(i, nam))\n",
    "\n",
    "\n",
    "# Process each raw file \n",
    "for i in os.listdir(Raw_data_copy): \n",
    "    \n",
    "    # For the excells\n",
    "    if i.endswith('.xlsx'):\n",
    "        thefile = os.path.join(Raw_data_copy, i)\n",
    "        # clean the header and read in  the data  (using one of custom functions\n",
    "        data = xls_cleaner(thefile)              \n",
    "        nam = i.split(\".\")[0]                    # remove the .xls\n",
    "        \n",
    "        # save to same filder but as a csv now.\n",
    "        data.to_csv(os.path.join(Raw_data_copy, nam+'.csv'), encoding='utf-8') # note: this step is where duplicate files will dissapear as they will be named the exact same thing and then ocerwitten by the loop \n",
    "        print(\"Converting {} to {}.csv\".format(i, nam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Aasu_2015-07-21_to_2016-01-19.csv\n",
      "Cleaning Aasu_2016-12-15_to_2017-01-07.csv\n",
      "Cleaning Aasu_2016-08-12_to_2016-09-28.csv\n",
      "Cleaning Aasu_2016-01-01_to_2016-08-12.csv\n",
      "Cleaning Aasu_2016-01-01_to_2016-11-28.csv\n",
      "Cleaning Aasu_2016-01-01_to_2016-12-15.csv\n",
      "Cleaning Aasu_2015-07-21_to_2015-10-06.csv\n",
      "Cleaning Aasu_2016-06-08_to_2016-07-18.csv\n",
      "Cleaning Aasu_2017-04-25_to_2018-01-29.csv\n",
      "Cleaning Alava_2017-07-01_to_2018-01-10.csv\n",
      "Cleaning Alava_2016-12-20_to_2017-01-19.csv\n",
      "Cleaning Alava_2017-05-01_to_2017-08-02.csv\n",
      "Cleaning Alava_2016-08-16_to_2016-09-20.csv\n",
      "Cleaning Alava_2016-01-01_to_2016-11-29.csv\n",
      "Cleaning Alava_2016-01-01_to_2016-12-20.csv\n",
      "Cleaning Alava_2017-01-01_to_2017-06-07.csv\n",
      "Cleaning Alava_2017-05-01_to_2017-10-11.csv\n",
      "Cleaning Alava_2015-07-30_to_2015-10-08.csv\n",
      "Cleaning ASPA_2016-08-18_to_2016-10-06.csv\n",
      "Cleaning ASPA_2016-05-26_to_2016-06-07.csv\n",
      "Cleaning ASPA_2016-04-08_to_2016-08-18.csv\n",
      "Cleaning ASPA_2016-04-08_to_2016-11-30.csv\n",
      "Cleaning ASPA_2016-04-08_to_2016-12-19.csv\n",
      "Cleaning ASPA_2016-06-07_to_2016-07-19.csv\n",
      "Cleaning ASPA_2015-07-25_to_2015-11-12.csv\n",
      "Cleaning ASPA_2016-12-19_to_2017-03-27.csv\n",
      "Cleaning ASPA_2016-12-19_to_2017-05-31.csv\n",
      "Cleaning ASPA_2017-01-01_to_2017-05-31.csv\n",
      "Cleaning ASPA_2016-12-01_to_2017-08-31.csv\n",
      "Cleaning ASPA_2017-04-16_to_2017-08-31.csv\n",
      "Cleaning Fagaitua_2017-07-01_to_2017-11-29.csv\n",
      "Cleaning Fagaitua_2017-02-21_to_2017-08-24.csv\n",
      "Cleaning Fagaitua_2017-06-01_to_2017-09-29.csv\n",
      "Cleaning Fagaitua_2017-02-21_to_2017-07-03.csv\n",
      "Cleaning Fangaitua_2017-04-20_to_2017-06-12.csv\n",
      "Cleaning Poloa_2017-07-01_to_2017-12-12.csv\n",
      "Cleaning Poloa_2016-12-15_to_2017-01-09.csv\n",
      "Cleaning Poloa_2017-02-16_to_2017-05-19.csv\n",
      "Cleaning Poloa_2017-02-16_to_2017-06-09.csv\n",
      "Cleaning Poloa_2017-01-01_to_2017-08-31.csv\n",
      "Cleaning Poloa_2016-08-18_to_2016-10-03.csv\n",
      "Cleaning Poloa_2016-01-01_to_2016-08-18.csv\n",
      "Cleaning Poloa_2016-01-01_to_2016-11-28.csv\n",
      "Cleaning Poloa_2016-01-01_to_2016-12-15.csv\n",
      "Cleaning Poloa_2017-06-01_to_2017-10-23.csv\n",
      "Cleaning Poloa_2015-07-21_to_2015-07-22.csv\n",
      "Cleaning Poloa_2015-07-21_to_2015-10-06.csv\n",
      "Cleaning Poloa_2016-06-08_to_2016-07-18.csv\n",
      "Cleaning Vaipito_2016-06-08_to_2016-06-16.csv\n",
      "Cleaning Vaipito_2016-05-17_to_2016-05-18.csv\n",
      "Cleaning Vaipito_2016-06-17_to_2016-07-18.csv\n",
      "Cleaning Vaipito_2015-07-30_to_2015-09-27.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2015-07-31.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2015-07-22.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2016-01-19.csv\n",
      "Cleaning Vaipito_2015-08-01_to_2015-08-25.csv\n",
      "Cleaning Vaipito_2015-09-02_to_2015-09-27.csv\n",
      "Cleaning Vaipito_2016-06-08_to_2016-06-16.csv\n",
      "Cleaning Vaipito_2016-06-08_to_2016-06-16.csv\n",
      "Cleaning Vaipito_2016-05-17_to_2016-05-18.csv\n",
      "Cleaning Vaipito_2015-07-30_to_2015-09-27.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2015-07-31.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2015-07-22.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2016-01-19.csv\n",
      "Cleaning Vaipito_2015-08-01_to_2015-08-25.csv\n",
      "Cleaning Vaipito_2015-09-02_to_2015-09-27.csv\n",
      "Cleaning Vaipito_2015-07-30_to_2015-09-27.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2015-07-31.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2015-07-22.csv\n",
      "Cleaning Vaipito_2015-07-21_to_2016-01-19.csv\n",
      "Cleaning Vaipito_2015-08-01_to_2015-08-25.csv\n",
      "Cleaning Vaipito_2015-09-02_to_2015-09-27.csv\n",
      "Cleaning Vipito_2016-06-17_to_2016-07-18.csv\n"
     ]
    }
   ],
   "source": [
    "# Process CSV files\n",
    "\n",
    "p = re.compile('time', re.IGNORECASE)  # string to look for is anything with \"time\" in it for taking out trash headings\n",
    "headings = ['DateTime', 'SRD_wpm2', 'HMD_pct', 'TMP_F', 'RNF_in', 'WND_deg', 'WNS_mph']  # Final data headings\n",
    "\n",
    "\n",
    "# Process each csv file \n",
    "for i in files1:        \n",
    "    # For the csvs\n",
    "    if i.endswith('.csv'):\n",
    "        tempdata_good =pd.DataFrame(columns = headings)                                  # blank dataframe to write to\n",
    "\n",
    "        with open(os.path.join(Raw_data_copy, i), 'r') as f_in:                 # open file \n",
    "            text_line_data = f_in.read().splitlines(True)                       # temporary object for finding header\n",
    "\n",
    "        for skips in range(0,6):\n",
    "            matcher = p.search(text_line_data[skips])                           # find the row that has the word time in it\n",
    "            if matcher:\n",
    "                rows2skip = skips                                               # ID the number of rows to skip to make the time row the header row\n",
    "        tempdata = pd.read_csv(os.path.join(Raw_data_copy, i), skiprows=rows2skip)  # read in dataset, with the correct header\n",
    "\n",
    "        tempdata.columns = tempdata.columns.str.strip()                         # remove any whitespace in the collumn headdings \n",
    "\n",
    "    # this block standardizes all the columns to the same names\n",
    "        for col in tempdata.columns:\n",
    "            if col in (\"TIMESTAMP\",\"Timestamp\", \"Date and Time\" ):\n",
    "                tempdata =  tempdata.rename(columns={col: \"DateTime\"} )\n",
    "            if col in (\"TMP\" , \"Air Temp (F)\", \"AirTF_Avg\"):\n",
    "                tempdata =  tempdata.rename(columns={col: \"TMP_F\"} )\n",
    "            if col in (\"HMD\", \"RH\", \"Relative Humidity\"):\n",
    "                tempdata =  tempdata.rename(columns={col: \"HMD_pct\"} )\n",
    "            if col in (\"SRD\", \"Solar Rad (W_AVG)\", \"SlrMJ_Tot\"):\n",
    "                tempdata =  tempdata.rename(columns={col: \"SRD_wpm2\"} )\n",
    "            if col in (\"RNF\", \"Rain_in_Tot\", \"Rain (in)\"):\n",
    "                tempdata =  tempdata.rename(columns={col: \"RNF_in\"} )\n",
    "            if col in (\"WND\", \"WindDir_D1_WVT\", \"Wind Direction (D1)\"):\n",
    "                tempdata =  tempdata.rename(columns={col: \"WND_deg\"} )       \n",
    "            if col in (\"WNS\", \"Wind Speed (MPH)\", \"WS_mph_S_WVT\"):\n",
    "                tempdata =  tempdata.rename(columns={col: \"WNS_mph\"} )  \n",
    "\n",
    "    # This block takes out unwanted headings and resets index\n",
    "        try:\n",
    "            for col2 in headings:\n",
    "                tempdata_good[col2] = tempdata[col2]                                    # select only the headings we want\n",
    "        except:                                                    # this except pass keeps it from hanging up where there are missing columns like in the ASPA SR station   \n",
    "            pass\n",
    "        tempdata_good = tempdata_good.reset_index(drop=True)                       # reset the row index to start from 0\n",
    "\n",
    "    #this names the data key based on its first and last date (if there is a repeat file it will not make a repeat data entry)\n",
    "        i = i.replace(\" \", \"_\")\n",
    "        i = i.replace(\"-\", \"_\")                # this takes any dashes and underscores turns them to spaces so that the first word can be used for the name even is there is a dash\n",
    "\n",
    "        tempdata_good['DateTime'] = pd.to_datetime(tempdata_good['DateTime'], errors='coerce')  # string date to real date\n",
    "    # note: some of the time stamps from the stations are not perfectily on 15 mins, this rounds to the nearest 15    \n",
    "\n",
    "        tempdata_good['DateTime'] = tempdata_good['DateTime'].apply(lambda dt: datetime(dt.year, dt.month, dt.day, dt.hour,15*(dt.minute // 15)))\n",
    "\n",
    "        nam = i.split(\"_\")[0] +'_'+ str(tempdata_good['DateTime'].iloc[0]).split(\" \")[0] +'_to_'+str(tempdata_good['DateTime'].iloc[-1]).split(\" \")[0]\n",
    "        \n",
    "        data = tempdata_good.copy()\n",
    "            \n",
    "        data.to_csv(os.path.join(Formatted_data, nam+'.csv'), encoding='utf-8') # note: this step is where duplicate files will dissapear as they will be named the exact same thing and then ocerwitten by the loop \n",
    "        print(\"Cleaning {}.csv\".format(nam))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xls_cleaner(file_2_read, datetime_name = [\"Date and Time\", \"Timestamp\",  \"TIMESTAMP\"] ):\n",
    "\n",
    "    data_xls = pd.read_excel(file_2_read,  header=None, index_col=None)                        # read  xlsx initially with no cols or index\n",
    "    data_xls_trimmed = data_xls.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)   # remove whitespace from all strigs\n",
    "    Col_true = data_xls_trimmed[data_xls_trimmed.isin(datetime_name)]                          # find the cell that has the start of the header   Note the datetime col has to always be ahead of data columns\n",
    "    Col_true = Col_true.dropna(how='all')                                                      # retain and select the index and col numbers of the cell with the start of the header\n",
    "    finally_frame = Col_true.dropna(axis = 1, how='all')\n",
    "    rows_to_skip = finally_frame.index[0]\n",
    "    index_col_number = finally_frame.columns[0]\n",
    "\n",
    "    data = pd.read_excel(file_2_read,  skiprows=rows_to_skip,  index_col=index_col_number)  # read  xlsx to memory\n",
    "\n",
    "    return data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
